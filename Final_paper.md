# Forecasting Monthly Player Activity for Live Service Games

## Adilet Shabay

Live service games depend on recurring content drops and seasonal events to maintain engagement, but monthly player activity can still change quickly due to updates, holidays, and shifts in public attention. Forecasting these changes can be useful for planning, such as scheduling updates, allocating support, and anticipating demand. In this project, I build a simple and reproducible workflow that predicts monthly average player activity for a set of multiplayer and live service games. The predictive task is multi step forecasting, where the model generates the next 12 months of monthly average players. I go beyond historical player counts by adding lightweight factor signals that could help explain engagement changes, including update related news activity, major seasonal calendar indicators, and a proxy for public interest. This was my first time building an end to end forecasting pipeline from scraping to modeling, and it was honestly interesting to see how quickly a “clean model idea” becomes a real data problem once you start collecting signals from the web.

The dataset is a monthly panel for 20 games built from three sources. Monthly average player counts are collected from SteamCharts. A proxy for update intensity is constructed using the Steam News API by retrieving each game’s news posts and counting posts per month that match update and event related keywords. A proxy for public attention is created using Google Trends monthly interest for each game name. These sources are merged into a continuous monthly timeline per game. Missing external signals are handled through conservative filling or forward filling, and target gaps are handled minimally to keep the panel usable for forecasting. After merging, I create model ready features such as recent rolling summaries of update and attention signals and calendar indicators for major seasonal periods. I also verify that rolling features are shifted so the model only uses information that would have been available at prediction time.

The analysis notebook converts the forecasting problem into a supervised learning setup. For each game, I create rolling examples where the inputs include a fixed lookback window of recent player activity plus factor features, and the output is the next 12 months of player counts. Player counts are modeled on a log scale to make large and small games more comparable and to stabilize training. I evaluate three simple baseline forecasting methods as reference points. The naive baseline repeats the last observed month across the future. The seasonal naive baseline repeats values from the same month one year earlier. The moving average baseline predicts using the average of the last three observed months. The main predictive model is a small neural network that takes the engineered feature vector and outputs a 12 month forecast in one pass. Inputs are standardized using training only statistics. The model is trained using a standard regression loss, and training is stopped early using validation performance to reduce overfitting. The data is split using time based boundaries so that evaluation reflects a real forecasting setting rather than random shuffling.

The results show that the pipeline works correctly end to end, but the trained model is not clearly better than simple baselines in a consistent way. On the held out test set, the naive baseline and the three month moving average baseline perform strongly. They achieve similar overall error levels, and they are surprisingly hard to beat, which was a little humbling to see the first time. The seasonal naive baseline performs noticeably worse, which suggests that repeating last year’s exact month is too rigid for these games, likely because updates and one off events change the shape of the series over time. The neural model improves one important aspect, which is reducing large squared mistakes. In simple terms, it helps limit some very big misses, which shows up in a lower RMSE compared with the baselines. However, on relative style metrics that measure typical percentage error across games of different sizes, the neural model does not outperform the best baselines. For example, the test sMAPE for the naive baseline is around 0.23, while the neural model is around 0.27, meaning the baseline is better on average percentage style error. The precision style table tells a similar story. On the test set, the naive baseline is within 30 percent error about three quarters of the time, while the neural model is closer to about two thirds.

The forecast plots also help interpret what is happening. The neural model tends to produce smoother predictions that follow the general level of the game, but it often fails to match sharper rises and drops in the actual data. That is not surprising given the features are only proxies. Counting update news posts and using public interest scores does not fully capture whether an update was huge or minor, or whether a sudden spike came from something external like esports events, a streamer trend, or a competitor release. So the model learns the general patterns and some seasonality, but it cannot reliably anticipate abrupt changes. Overall, I would describe the neural model as “working” and sometimes helpful for avoiding extreme mistakes, but not strong enough to claim it is a better predictor than simple baselines for this dataset.

The main limitations come from the quality and completeness of the factor information. Update intensity is approximated using keyword matching on news posts, which can be noisy and does not measure the scale of an update. Google Trends is an indirect proxy for attention and does not always translate into actual gameplay. Several important drivers are not included, such as exact patch calendars, esports tournaments, marketing campaigns, competitor releases, and streaming engagement on platforms like Twitch or YouTube. The modeling approach is intentionally simple and pooled across games, which can encourage smoothing and regression toward typical behavior. Finally, the evaluation summarizes performance across all forecast months, which can hide the fact that forecasts usually degrade the farther into the future they go. To improve the model, the most direct upgrades would be to replace proxies with more structured event data, such as official patch notes and known season start dates, and to add stronger attention signals like Twitch viewership, YouTube trends, or esports event schedules. It would also help to evaluate accuracy separately for each forecast month to understand where performance drops, and to try either per game models or a slightly more sequence aware forecasting model that can better represent temporal dynamics without relying on heavy complexity.

This project demonstrates an end to end workflow for forecasting monthly player activity using a dataset built from SteamCharts and enriched with update, seasonality, and attention proxies. The baseline methods set a strong standard, and the neural model does not consistently beat them, even though it can reduce some large errors. The most practical conclusion is that, with the current signals and a simple model, it is difficult to outperform naive and moving average forecasts for this task. At the same time, the model is still useful as a structured way to combine multiple signals into a single forecasting system, and it provides a foundation that can realistically improve if higher quality event information and richer popularity signals are added. Next steps that would most likely improve results are adding more precise update and event metadata, integrating streaming based signals, reporting accuracy by forecast month, and exploring either per game models or lightweight sequence based approaches to better capture game specific behavior.
